{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foundations of Data Mining: Assignment 1\n",
    "\n",
    "Please complete all assignments in this notebook. You should submit this notebook, as well as a PDF version (See File > Download as)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'preamble'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-0a051554f729>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[1;32mfrom\u001b[0m \u001b[0mpreamble\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrcParams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'savefig.dpi'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mInteractiveShell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mast_node_interactivity\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"all\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'preamble'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from preamble import *\n",
    "plt.rcParams['savefig.dpi'] = 100\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handwritten digit recognition (5 points, 1+2+2)\n",
    "The [MNIST dataset](https://www.openml.org/d/554) contains 70,000 images of handwritten digits (0-9) represented by 28 by 28 pixel values. We can easily download it from OpenML and visualize one of the examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'oml' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-34be99a87533>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;31m# This is a temporary read-only OpenML key. Replace with your own key later.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0moml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapikey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'88854e9607bcd390ff54881a727f43af'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'oml' is not defined"
     ]
    }
   ],
   "source": [
    "# This is a temporary read-only OpenML key. Replace with your own key later. \n",
    "oml.config.apikey = '88854e9607bcd390ff54881a727f43af'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'oml' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-cbe767bdebab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmnist_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m554\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Download MNIST data\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmnist_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmnist_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefault_target_attribute\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m \u001b[1;31m# Get the predictors X and the labels y\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m28\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgray_r\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Take the first example, reshape to a 28x28 image and plot\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Class label:\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Print the correct class label\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m28\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgray_r\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Take the first example, reshape to a 28x28 image and plot\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'oml' is not defined"
     ]
    }
   ],
   "source": [
    "mnist_data = oml.datasets.get_dataset(554) # Download MNIST data\n",
    "X, y = mnist_data.get_data(target=mnist_data.default_target_attribute); # Get the predictors X and the labels y\n",
    "plt.imshow(X[1].reshape(28, 28), cmap=plt.cm.gray_r) # Take the first example, reshape to a 28x28 image and plot\n",
    "print(\"Class label:\",y[1]) # Print the correct class label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Evaluate a k-Nearest Neighbor classifier with its default settings.\n",
    "    - Use the first 60,000 examples as the training set and the last 10,000 as the test set\n",
    "    - What is the predictive accuracy?\n",
    "    - Find a few misclassifications, and plot them together with the true labels (as above). Are these images really hard to classify?\n",
    "- Optimize the value for the number of neighbors $k$ (keep $k$ < 50) on a stratified subsample (e.g. 10%) of the data\n",
    "    - Use 10-fold crossvalidation and plot $k$ against the misclassification rate. Which value of $k$ should you pick?\n",
    "    - Do the same but with 100 bootstrapping repeats. Are the results different? Explain.\n",
    "- Compare kNN against the linear classification models that we have covered in the course (logistic regression and linear SVMs).\n",
    "    - First use the default hyperparameter settings.\n",
    "    - Next, optimize for the degree of regularization ($C$) and choice of penalty (L1/L2). Again, plot the accuracy while increasing the degree of regularization for different penalties. Interpret the results. \n",
    "    - Report is the optimal performance. Can you get better results than kNN?\n",
    "    \n",
    "Report all results clearly and interpret the results.  \n",
    "Note: while prototyping/bugfixing, you can speed up experiments by taking a smaller sample of the data, but report your results as indicated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN score: 0.932813\n",
      "LogisticRegression score: 0.821609\n"
     ]
    }
   ],
   "source": [
    "from sklearn import neighbors, linear_model\n",
    "\n",
    "X_train, X_test = np.split(X,[6000])\n",
    "y_train, y_test = np.split(y,[6000])\n",
    "knn = neighbors.KNeighborsClassifier()\n",
    "logistic = linear_model.LogisticRegression()\n",
    "\n",
    "print('KNN score: %f' % knn.fit(X_train, y_train).score(X_test, y_test))\n",
    "print('LogisticRegression score: %f'\n",
    "      % logistic.fit(X_train, y_train).score(X_test, y_test))\n",
    "pred=knn.predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN score: 0.932813\n",
    "LogisticRegression score: 0.821609"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection (4 points (2+2))\n",
    "Study how RandomForest hyperparameters interact on the Ionosphere dataset (OpenML ID 59).\n",
    "\n",
    "- Optimize a RandomForest, varying both $n\\_estimators$ and $max\\_features$ at the same time. Use a nested cross-validation and a grid search (or random search) over the possible values, and measure the AUC. Explore how fine-grained this grid/random search can be, given your computational resources. What is the optimal AUC performance you find?\n",
    "- Again, vary both hyperparameters, but this time use a grid search and visualize the results as a plot (heatmap) $n\\_estimators \\times max\\_features \\rightarrow AUC$ with AUC visualized as the color of the data point. Try to make the grid as fine as possible. Interpret the results. Can you explain your observations? What did you learn about tuning RandomForests?\n",
    "\n",
    "Hint: Running this experiment can take a while, so start early and use a feasible grid/random search. Start with a coarse grid or few random search iterations.\n",
    "Hint: Use a log scale (1,2,4,8,16,...) for $n\\_estimators$. Vary $max\\_features$ linearly between 1 and the total number of features. Note that, if you give $max\\_features$ a float value, it will use it as [the percentage of the total number of features](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ionosphere = oml.datasets.get_dataset(59) # Download Ionosphere data\n",
    "X, y = ionosphere.get_data(target=ionosphere.default_target_attribute); # Get the predictors X and the labels y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree heuristics (1 point)\n",
    "Consider the toy training set created below. It predicts whether your date agrees to go out with you depending on the weather.\n",
    "\n",
    "Learn a decision tree:\n",
    "\n",
    "- Implement functions to calculate entropy and information gain\n",
    "- What is the class entropy for the entire dataset? What is the information gain when you split the data using the *Water* feature?\n",
    "- Implement a basic decision tree:\n",
    "    - Select a feature to split on according to its information gain. If multiple features are equally good, select the leftmost one.\n",
    "    - Split the data and repeat until the tree is complete.\n",
    "    - Print out the results (nodes and splits).\n",
    "- Now train a scikit-learn decision tree on the same data. Do you get the same result? Explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"Sky\":['sunny','sunny','rainy','sunny','sunny'],\n",
    "                   \"AirTemp\":['warm','warm','warm','cold','warm'],\n",
    "                   \"Humidity\":['normal','high','high','high','normal'],\n",
    "                   \"Wind\":['strong','strong','strong','strong','weak'],\n",
    "                   \"Water\":['warm','warm','cool','warm','warm'],\n",
    "                   \"Forecast\":['same','same','change','change','same'],\n",
    "                   \"Date?\":['yes','yes','no','yes','no']\n",
    "                   });\n",
    "df = df[['Sky', 'AirTemp', 'Humidity', 'Wind', 'Water', 'Forecast', 'Date?']] # Fix column ordering\n",
    "df # print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Complete these functions first\n",
    "def entropy(pos, neg):\n",
    "    return 0\n",
    "\n",
    "def info_gain(pos1,neg1,pos2,neg2):\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests (4 points (1+1+2))\n",
    "Study the effect of the number of trees in a RandomForest on the EEG-eye-state dataset (http://www.openml.org/d/1471). This dataset measures brain activity using 15 sensors, and you need to predict whether the person's eyes are open or closed. \n",
    "\n",
    "* Train a RandomForest classifier on this dataset with an increasing number of trees (on a log scale as above). Plot the Out-Of-Bag error against the number of trees.\n",
    "    - The Out-Of-Bag error is the test error obtained when using bootstrapping, and using the non-drawn data points as the test set. This is what a RandomForest does internally, so you can retrieve it from the classifier. The code below hints on how to do this.\n",
    "* Construct the same plot, but now use 10-fold Cross-validation and error rate instead of the OOB error. Compare the two. What do you learn from this?\n",
    "* Compare the performance of the RandomForest ensemble with that of a single full decision tree. Compute the AUC as well as the bias and variance. Does the bias and variance increase/decrease for the ensemble? Does the number of trees affect the result?\n",
    "\n",
    "Hint: Error rate = 1 - accuracy  \n",
    "Hint: We discussed bias-variance decomposition in class. It is not included in scikit-learn, so you'll need to implement it yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eeg = oml.datasets.get_dataset(1471) # Download Ionosphere data\n",
    "X, y = eeg.get_data(target=eeg.default_target_attribute);\n",
    "\n",
    "# Out of bag errors can be retrieved from the RandomForest classifier. You'll need to loop over the number of trees.\n",
    "# http://scikit-learn.org/stable/auto_examples/ensemble/plot_ensemble_oob.html\n",
    "from sklearn import ensemble\n",
    "clf = ensemble.RandomForestClassifier()\n",
    "clf.fit(X, y)\n",
    "(1 - clf.oob_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A regression benchmark (1 point)\n",
    "Consider the liver-disorder dataset (http://www.openml.org/d/8). The goal is to predict how much alcohol someone consumed based on blood test values.\n",
    "\n",
    "- Take a selection of the algorithms that we covered in class that can do regression.\n",
    "- Based on what you learned in the previous exercises, make educated guesses about good hyperparameter values and set up a grid or random search.\n",
    "- Evaluate all models with 10-fold cross-validation and root mean squared error (RMSE). Report all results. Which model yields the best results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "liver = oml.datasets.get_dataset(8) # Download Liver-disorders data\n",
    "X, y = liver.get_data(target=liver.default_target_attribute);"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
